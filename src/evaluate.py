import torch
import argparse
import os
import matplotlib.pyplot as plt
import sacrebleu
import sentencepiece as spm
import google.generativeai as genai
from tqdm import tqdm
from datasets import load_dataset
from model import Transformer


# import getpass

# os.environ["GEMINI_API_KEY"] = getpass.getpass("Enter your Gemini API key: ")


GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

print(GEMINI_API_KEY)


def get_args():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--checkpoint", type=str, required=True, help="Path to best_transformer.pth"
    )
    parser.add_argument("--spm_model", type=str, default="models/spm.model")
    parser.add_argument(
        "--device", type=str, default="cuda" if torch.cuda.is_available() else "cpu"
    )
    parser.add_argument(
        "--beam_size", type=int, default=3, help="Beam size for decoding"
    )
    parser.add_argument("--max_len", type=int, default=128)
    parser.add_argument(
        "--test_samples",
        type=int,
        default=100,
        help="S·ªë l∆∞·ª£ng c√¢u test (ƒë·ª´ng test h·∫øt n·∫øu d√πng Gemini)",
    )

    # Model params (Ph·∫£i kh·ªõp v·ªõi l√∫c train)
    parser.add_argument("--vocab_size", type=int, default=7000)
    parser.add_argument("--embed_dim", type=int, default=128)
    parser.add_argument("--heads", type=int, default=4)
    parser.add_argument("--layers", type=int, default=3)
    parser.add_argument("--ff_expansion", type=int, default=4)
    parser.add_argument("--pad_idx", type=int, default=0)

    return parser.parse_args()


# --- 1. BEAM SEARCH DECODING (ENGINEER LEVEL) ---
def beam_search_decode(model, src, sp, device, beam_size=3, max_len=128):
    """
    Thu·∫≠t to√°n Beam Search ƒë·ªÉ t√¨m b·∫£n d·ªãch t·ªët nh·∫•t.
    """
    model.eval()
    bos_id = sp.bos_id()
    eos_id = sp.eos_id()

    # Encoder output ch·ªâ c·∫ßn t√≠nh 1 l·∫ßn
    with torch.no_grad():
        src_mask = model.make_src_mask(src)
        enc_out = model.dropout(model.pos_encoder(model.embedding(src)))
        for layer in model.transformer_encoder:
            enc_out = layer(enc_out, src_mask)
        # Final Norm encoder
        enc_out = model.final_norm(enc_out)

    # Kh·ªüi t·∫°o Beam: M·ªói beam ch·ª©a (sequence, score)
    # Sequence b·∫Øt ƒë·∫ßu b·∫±ng [BOS]
    k_candidates = [(torch.tensor([bos_id], dtype=torch.long, device=device), 0.0)]

    # Loop cho ƒë·∫øn max_len
    for _ in range(max_len):
        new_candidates = []

        for seq, score in k_candidates:
            # N·∫øu c√¢u ƒë√£ k·∫øt th√∫c b·∫±ng EOS, gi·ªØ nguy√™n
            if seq[-1].item() == eos_id:
                new_candidates.append((seq, score))
                continue

            # Forward Decoder
            # L∆∞u √Ω: Pass seq shape [1, Len] v√†o
            tgt_input = seq.unsqueeze(0)
            tgt_mask = model.make_trg_mask(tgt_input)

            dec_out = model.dropout(model.pos_encoder(model.embedding(tgt_input)))
            for layer in model.transformer_decoder:
                dec_out = layer(dec_out, tgt_mask, context=enc_out, src_mask=src_mask)

            dec_out = model.final_norm(dec_out)
            out = model.linear(dec_out)  # [1, Len, Vocab]

            # L·∫•y x√°c su·∫•t c·ªßa token cu·ªëi c√πng
            # D√πng LogSoftmax ƒë·ªÉ c·ªông ƒëi·ªÉm cho d·ªÖ (thay v√¨ nh√¢n x√°c su·∫•t)
            probs = torch.log_softmax(out[:, -1, :], dim=-1).squeeze()

            # L·∫•y top-k token t·ªët nh·∫•t ti·∫øp theo
            topk_probs, topk_ids = torch.topk(probs, beam_size)

            for i in range(beam_size):
                token = topk_ids[i]
                prob = topk_probs[i].item()

                # T·∫°o sequence m·ªõi
                new_seq = torch.cat([seq, token.unsqueeze(0)], dim=0)
                new_score = score + prob  # C·ªông log prob
                new_candidates.append((new_seq, new_score))

        # S·∫Øp x·∫øp t·∫•t c·∫£ candidates theo score gi·∫£m d·∫ßn v√† l·∫•y top k
        k_candidates = sorted(new_candidates, key=lambda x: x[1], reverse=True)[
            :beam_size
        ]

        # N·∫øu t·∫•t c·∫£ c√°c beam ƒë·ªÅu ƒë√£ g·∫∑p EOS th√¨ d·ª´ng s·ªõm
        if all(c[0][-1].item() == eos_id for c in k_candidates):
            break

    # L·∫•y sequence c√≥ ƒëi·ªÉm cao nh·∫•t
    best_seq = k_candidates[0][0]
    return best_seq.cpu().tolist()


# --- 2. GEMINI SCORE (LLM-AS-A-JUDGE) ---
def get_gemini_score(source, reference, candidate):
    """
    D√πng AI ch·∫•m ƒëi·ªÉm AI. G·ª≠i prompt l√™n Google Gemini.
    Tr·∫£ v·ªÅ ƒëi·ªÉm 0-100.
    """
    if not GEMINI_API_KEY or GEMINI_API_KEY == "YOUR_GEMINI_API_KEY":
        return None  # B·ªè qua n·∫øu kh√¥ng c√≥ key

    genai.configure(api_key=GEMINI_API_KEY)  # type: ignore
    model = genai.GenerativeModel("models/gemini-2.5-flash")  # type: ignore

    prompt = f"""
    You are a professional translator. Evaluate the quality of the translation from English to Vietnamese.
    
    Source (English): "{source}"
    Reference (Vietnamese): "{reference}"
    Candidate (Machine Translation): "{candidate}"
    
    Score the Candidate translation on a scale from 0 to 100 based on accuracy, fluency, and meaning preservation.
    Return ONLY the number.
    """

    try:
        response = model.generate_content(prompt)
        score = int(response.text.strip())
        return score
    except Exception as e:
        print(f"Error: {e}")
        return 50  # Fallback n·∫øu l·ªói


# --- 3. MAIN EVALUATION LOOP ---
def main():
    args = get_args()

    # 1. Load Tokenizer & Data
    print("‚è≥ Loading Tokenizer & Data...")
    sp = spm.SentencePieceProcessor()
    sp.load(args.spm_model)  # type: ignore

    # Load test set (d√πng t·∫≠p test th·∫≠t c·ªßa opus100)
    dataset = load_dataset("opus100", "en-vi", split=f"test[:{args.test_samples}]")

    # 2. Load Model
    print(f"üèóÔ∏è Loading Model from {args.checkpoint}...")
    model = Transformer(
        vocab_size=args.vocab_size,
        embed_dim=args.embed_dim,
        heads=args.heads,
        blocks=args.layers,
        dropout=0.0,  # Eval mode kh√¥ng c·∫ßn dropout
        ff_expansion=args.ff_expansion,
        device=args.device,
    ).to(args.device)

    state_dict = torch.load(args.checkpoint, map_location=args.device)
    model.load_state_dict(state_dict)

    # 3. Running Evaluation
    sources = []
    references = []
    candidates = []
    gemini_scores = []

    print(f"üöÄ Starting Evaluation with Beam Size = {args.beam_size}...")

    for item in tqdm(dataset):
        src_text = item["translation"]["en"]
        tgt_text = item["translation"]["vi"]

        # Tokenize Source
        # (L∆∞u √Ω: Kh√¥ng c·∫ßn padding batch ·ªü ƒë√¢y v√¨ ta decode t·ª´ng c√¢u m·ªôt cho ch√≠nh x√°c)
        src_ids = [sp.bos_id()] + sp.encode_as_ids(src_text) + [sp.eos_id()]  # type: ignore
        src_tensor = (
            torch.tensor(src_ids, dtype=torch.long).unsqueeze(0).to(args.device)
        )

        # --- BEAM SEARCH ---
        pred_ids = beam_search_decode(
            model,
            src_tensor,
            sp,
            args.device,
            beam_size=args.beam_size,
            max_len=args.max_len,
        )

        # Decode v·ªÅ text
        # L·ªçc b·ªè special tokens
        pred_text = sp.decode(pred_ids)  # type: ignore

        sources.append(src_text)
        references.append(tgt_text)
        candidates.append(pred_text)

        # Ch·∫•m ƒëi·ªÉm Gemini (Optional - t·ªën ti·ªÅn/quota)
        # Ch·ªâ ch·∫•m 10 c√¢u ƒë·∫ßu ƒë·ªÉ demo
        if len(gemini_scores) < 10:
            g_score = get_gemini_score(src_text, tgt_text, pred_text)
            if g_score is not None:
                gemini_scores.append(g_score)

    # 4. Compute BLEU
    # SacreBLEU expects references as a list of lists: [[ref1_doc], [ref2_doc]...]
    print("Computing BLEU...")
    bleu = sacrebleu.corpus_bleu(candidates, [references])
    print(f"‚úÖ BLEU Score: {bleu.score:.2f}")

    if gemini_scores:
        avg_gemini = sum(gemini_scores) / len(gemini_scores)
        print(f"ü§ñ Avg Gemini Score (First 10 samples): {avg_gemini:.2f}/100")

    # 5. Visualization & Analysis
    print("üìä Plotting Metrics...")

    # 5.1 In th·ª≠ v√†i m·∫´u
    print("\n--- SAMPLE TRANSLATIONS ---")
    for i in range(5):
        print(f"Src: {sources[i]}")
        print(f"Ref: {references[i]}")
        print(f"Pred: {candidates[i]}")
        print("-" * 30)

    # 5.2 Plot BLEU analysis (Gi·∫£ l·∫≠p Loss v√¨ Loss Inference kh√¥ng quan tr·ªçng b·∫±ng BLEU)
    # Ta s·∫Ω v·∫Ω "ƒê·ªô d√†i c√¢u vs BLEU" - ƒë·ªÉ xem model d·ªãch c√¢u d√†i hay ng·∫Øn t·ªët h∆°n

    sent_lens = [len(ref.split()) for ref in references]
    # Chia bin ƒë·ªô d√†i: 0-10, 10-20, 20-30...
    bins = {}
    for i, length in enumerate(sent_lens):
        bin_idx = (length // 10) * 10
        if bin_idx not in bins:
            bins[bin_idx] = {"refs": [], "cands": []}
        bins[bin_idx]["refs"].append(references[i])
        bins[bin_idx]["cands"].append(candidates[i])

    sorted_bins = sorted(bins.keys())
    bleu_per_bin = []

    for b in sorted_bins:
        if not bins[b]["refs"]:
            bleu_per_bin.append(0)
            continue
        # T√≠nh BLEU cho t·ª´ng nh√≥m ƒë·ªô d√†i
        score = sacrebleu.corpus_bleu(bins[b]["cands"], [bins[b]["refs"]]).score
        bleu_per_bin.append(score)

    # V·∫Ω bi·ªÉu ƒë·ªì
    plt.figure(figsize=(10, 5))
    plt.bar([f"{b}-{b + 10}" for b in sorted_bins], bleu_per_bin, color="skyblue")
    plt.xlabel("Sentence Length (words)")
    plt.ylabel("BLEU Score")
    plt.title("Translation Quality vs Sentence Length")
    plt.grid(axis="y", linestyle="--", alpha=0.7)

    # L∆∞u ·∫£nh
    os.makedirs("results", exist_ok=True)
    plt.savefig("results/bleu_analysis.png")
    print("‚úÖ Saved plot to results/bleu_analysis.png")


if __name__ == "__main__":
    main()
